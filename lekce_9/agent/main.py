"""LangChain agent for Wazuh incident analysis."""
import asyncio
import os
from datetime import datetime
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain.agents import create_agent

from analyzer import (
    extract_incident_data_from_mcp_response,
    format_data_for_llm_analysis
)
from pdf_generator import generate_pdf_report

# Load environment variables
load_dotenv()


async def get_mcp_tools():
    """Load tools from Wazuh MCP server."""
    mcp_url = os.getenv("WAZUH_MCP_URL", "http://localhost:8002/mcp")

    client = MultiServerMCPClient({
        "wazuh": {
            "transport": "streamable_http",
            "url": mcp_url,
        }
    })

    tools = await client.get_tools()
    return tools, client


async def main():
    """Main agent workflow."""
    print("ğŸš€ Starting Wazuh Incident Analysis Agent...")

    # Configuration
    max_sample_size = int(os.getenv("MAX_INCIDENTS_SAMPLE", "1000"))
    report_output_dir = os.getenv("REPORT_OUTPUT_DIR", "./reports")
    logo_path = os.getenv("COMPANY_LOGO_PATH", "./logo-full-color-cropped.png")

    # Ensure output directory exists
    os.makedirs(report_output_dir, exist_ok=True)

    # 1. Connect to MCP server and load tools
    print("ğŸ“¡ Connecting to Wazuh MCP server...")
    mcp_tools, mcp_client = await get_mcp_tools()
    print(f"âœ… Loaded {len(mcp_tools)} tools from MCP server")

    # 2. Configure LLM (Ollama via LiteLLM proxy)
    print("ğŸ¤– Configuring LLM (Ollama llama3 via LiteLLM proxy)...")
    litellm_base_url = os.getenv("LITELLM_BASE_URL", "http://localhost:4000")
    litellm_api_key = os.getenv("LITELLM_API_KEY", "dummy-key")

    llm = ChatOpenAI(
        base_url=litellm_base_url,
        api_key=litellm_api_key,
        #model="ollama-llama3.1",
        #model="gpt-5-nano",
        #model="gpt-4.1",
        model="gemini-2.5-pro",
        temperature=0.7
    )

    # 3. Create LangChain agent
    print("ğŸ”§ Creating LangChain agent...")
    agent = create_agent(
        llm,
        tools=mcp_tools,
        system_prompt="""Jsi expert na kybernetickou bezpeÄnost a analÃ½zu bezpeÄnostnÃ­ch incidentÅ¯.
Analyzuj Wazuh bezpeÄnostnÃ­ incidenty a identifikuj vzory, trendy a anomÃ¡lie.
Poskytuj doporuÄenÃ­ jak strategickÃ¡ (high-level smÄ›Å™ovÃ¡nÃ­), tak taktickÃ¡ (konkrÃ©tnÃ­ technickÃ¡ opatÅ™enÃ­)
pro snÃ­Å¾enÃ­ poÄtu incidentÅ¯.
ZamÄ›Å™ se na: ÃºrovnÄ› zÃ¡vaÅ¾nosti, geografickÃ© vzory, typy ÃºtokÅ¯, postiÅ¾enÃ© servery.

DOSTUPNÃ‰ TOOLS:
- search_wazuh_incidents: VyhledÃ¡ Wazuh incidenty z OpenSearch. Parametry: days (int), max_sample_size (int), query_type (string).

PouÅ¾Ã­vej POUZE tyto dostupnÃ© tools. NEVYMÃÅ LEJ SI nÃ¡zvy tools.
VÅ¡echny odpovÄ›di a doporuÄenÃ­ piÅ¡ v ÄeÅ¡tinÄ›."""
    )

    # 4. Query incidents - call MCP tool directly to ensure correct parameters
    print(f"\nğŸ“Š Dotazuji incidenty za poslednÃ­ch 7 dnÃ­ (max {max_sample_size} vzorkÅ¯)...")

    try:
        # Call the MCP tool directly with exact parameters
        search_tool = None
        for tool in mcp_tools:
            if hasattr(tool, 'name') and 'search_wazuh_incidents' in tool.name:
                search_tool = tool
                break

        if not search_tool:
            print("âŒ Tool search_wazuh_incidents not found!")
            return

        # Invoke the tool directly with correct parameters
        mcp_response_text = await search_tool.ainvoke({
            "days": 7,
            "max_sample_size": max_sample_size,
            "query_type": "all"
        })

        print("âœ… Data zÃ­skÃ¡na z OpenSearch")

        # Parse incident data
        print("ğŸ” Analyzuji data...")
        incident_data = extract_incident_data_from_mcp_response(mcp_response_text)

        print(f"\nğŸ“ˆ Statistiky:")
        print(f"  - CelkovÃ½ poÄet incidentÅ¯: {incident_data['statistics']['total_incidents']}")
        print(f"  - DennÃ­ prÅ¯mÄ›r: {incident_data['statistics']['daily_average']}")
        print(f"  - KritickÃ© incidenty: {incident_data['statistics']['critical_count']}")
        print(f"  - ZemÄ› - nejvÄ›tÅ¡Ã­ zdroj incidentÅ¯: {incident_data['statistics']['top_country']}")

        # 5. Generate LLM analysis and recommendations
        print("\nğŸ§  Generuji analÃ½zu a doporuÄenÃ­ pomocÃ­ LLM...")

        formatted_data = format_data_for_llm_analysis(incident_data)

        # Call LLM directly without agent to avoid tool-calling behavior
        analysis_prompt = f"""Jsi expert na kybernetickou bezpeÄnost.

Na zÃ¡kladÄ› tÄ›chto Wazuh incidentÅ¯ napiÅ¡ analÃ½zu a doporuÄenÃ­ v PROSTÃ‰M TEXTU (ne JSON).

{formatted_data}

NapiÅ¡ odpovÄ›Ä ve formÃ¡tu:

STRUÄŒNÃ ANALÃZA WAZUH INCIDENTÅ®

[2-3 odstavce sumarizujÃ­cÃ­ bezpeÄnostnÃ­ situaci]

STRATEGICKÃ DOPORUÄŒENÃ

1. [PrvnÃ­ strategickÃ© doporuÄenÃ­]
2. [DruhÃ© strategickÃ© doporuÄenÃ­]
3. [TÅ™etÃ­ strategickÃ© doporuÄenÃ­]
4. [ÄŒtvrtÃ© strategickÃ© doporuÄenÃ­]
5. [PÃ¡tÃ© strategickÃ© doporuÄenÃ­]

TAKTICKÃ A TECHNICKÃ DOPORUÄŒENÃ

1. [PrvnÃ­ taktickÃ© doporuÄenÃ­ - konkrÃ©tnÃ­ IP adresy a servery]
2. [DruhÃ© taktickÃ© doporuÄenÃ­]
3. [TÅ™etÃ­ taktickÃ© doporuÄenÃ­]
4. [ÄŒtvrtÃ© taktickÃ© doporuÄenÃ­]
5. [PÃ¡tÃ© taktickÃ© doporuÄenÃ­]

PouÅ¾Ã­vej konkrÃ©tnÃ­ data - IP adresy, nÃ¡zvy serverÅ¯, ÄÃ­sla z analÃ½zy.
"""

        # Call LLM directly without agent framework
        from langchain_core.messages import HumanMessage

        llm_response = await llm.ainvoke([HumanMessage(content=analysis_prompt)])

        # Extract text from response
        if hasattr(llm_response, 'content'):
            analysis_text = llm_response.content
        else:
            analysis_text = str(llm_response)

        # Clean up if needed
        if not analysis_text or len(analysis_text) < 100:
            analysis_text = "NepodaÅ™ilo se vygenerovat analÃ½zu. LLM vrÃ¡tilo prÃ¡zdnou nebo pÅ™Ã­liÅ¡ krÃ¡tkou odpovÄ›Ä."

        print("âœ… AnalÃ½za dokonÄena")

        # 6. Generate PDF report
        print("\nğŸ“„ Generuji PDF report...")

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(report_output_dir, f"wazuh_report_{timestamp}.pdf")

        generate_pdf_report(
            incident_data=incident_data,
            analysis=analysis_text,
            output_file=output_file,
            logo_path=logo_path
        )

        print(f"\nâœ… HOTOVO! Report byl uloÅ¾en do: {output_file}")

    except Exception as e:
        print(f"\nâŒ Chyba bÄ›hem zpracovÃ¡nÃ­: {str(e)}")
        import traceback
        traceback.print_exc()

    finally:
        # Cleanup (MultiServerMCPClient doesn't have cleanup method)
        pass


if __name__ == "__main__":
    asyncio.run(main())
